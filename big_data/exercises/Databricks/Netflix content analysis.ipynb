{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a8125c-18b0-49e8-a21b-abb764ed4d52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Netflix popular movies dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/narayan63/netflix-popular-movies-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2dcebd3-f3bb-4178-b848-2325f0bb973b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## *1. Lectura de datos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78a3bd7-cfd0-4652-aec1-8bad40d07c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nombre de la base de datos en Databricks\n",
    "database_name = \"default\"\n",
    "# Nombre de la tabla que deseas leer\n",
    "table_name = \"netflix\"\n",
    "\n",
    "# Leer la tabla en un DataFrame de Spark\n",
    "df = spark.table(f\"{database_name}.{table_name}\")\n",
    "\n",
    "# Mostrar las primeras filas de la tabla\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e189df1-1c9d-4ce6-9542-fd920484f396",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## *2. Perfilamiento básico de datos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80416629-db75-4d1c-b31d-16e1ae7a323e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, stddev, min, max, countDistinct, approxCountDistinct\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType\n",
    "\n",
    "# Mostrar información general del DataFrame\n",
    "print(\"Información general del DataFrame:\")\n",
    "print(f\"Número total de filas: {df.count()}\")\n",
    "print(\"Número de columnas:\", len(df.columns))\n",
    "\n",
    "# Obtener información particular de cada columna\n",
    "print(\"\\nInformación de cada columna:\")\n",
    "for col_name in df.columns:\n",
    "    print(f\"\\nColumna: {col_name}\")\n",
    "    print(f\"Tipo de datos: {df.schema[col_name].dataType}\")\n",
    "    print(f\"Número de valores no nulos: {df.filter(col(col_name).isNotNull()).count()}\")\n",
    "    print(f\"Número de valores nulos: {df.filter(col(col_name).isNull()).count()}\")\n",
    "    \n",
    "    # Si la columna es numérica, calcular estadísticas descriptivas\n",
    "    if isinstance(df.schema[col_name].dataType, (IntegerType, LongType, DoubleType)):\n",
    "        stats = df.select(mean(col_name).alias('Mean'),\n",
    "                          stddev(col_name).alias('Stddev'),\n",
    "                          min(col_name).alias('Min'),\n",
    "                          max(col_name).alias('Max')).collect()[0]\n",
    "        print(f\"Media: {stats['Mean']}, Desviación estándar: {stats['Stddev']}\")\n",
    "        print(f\"Mínimo: {stats['Min']}, Máximo: {stats['Max']}\")\n",
    "        \n",
    "        # Histograma (opcional)\n",
    "        # df.select(col_name).rdd.flatMap(lambda x: x).histogram(10)\n",
    "        \n",
    "    # Si la columna es categórica, mostrar valores únicos y frecuencia\n",
    "    elif df.schema[col_name].dataType in ['StringType']:\n",
    "        unique_values = df.groupBy(col_name).count().orderBy('count', ascending=False).collect()\n",
    "        print(\"Valores únicos:\")\n",
    "        for val in unique_values:\n",
    "            print(f\"{val[col_name]}: {val['count']}\")\n",
    "        \n",
    "    # Mostrar algunos ejemplos de valores (opcional)\n",
    "    example_values = df.select(col_name).sample(False, 0.1).limit(5).collect()\n",
    "    print(\"Ejemplos de valores:\")\n",
    "    for row in example_values:\n",
    "        print(row[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d3c9ec-013f-4f9f-8510-d7b73eb335d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a38de4-7825-44fc-943c-08d155b249eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## *3. Transformaciones en los datos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75b84f75-c129-45dd-b34f-7652642b855c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 3.1 Columna \"genre\" (Género)\n",
    "\n",
    "Hemos evidenciado que la columna \"Genre\" tiene diferentes valores. Para facilidad de un análisis posterior, vamos a hacer una transformación a partir de este campo.\n",
    "\n",
    "La transformación implica que solo vamos a dejar el primer valor de esta columna, asumiendo que es su género principal.\n",
    "\n",
    "*La columna Genre, debe mantener su valor original.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5457d46-1d0d-44c3-962b-93ecf20fb64b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Dividir la cadena en una lista de valores, utilizando la función split y utilizando la \",\" como separador\n",
    "# Posteriormente, nos quedamos con el primer valor [0] de la lista generada\n",
    "# Este valor lo agregamos a la Columna \"MainGenre\"\n",
    "df = df.withColumn(\"MainGenre\", split(col(\"Genre\"), \",\")[0])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ee874a-87a8-4ede-b549-54e5c5a9c332",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 3.2 Columna \"duration\" (Duración)\n",
    "\n",
    "Hemos identificado que la columna \"duration\" contiene un error de calidad de datos bastante frecuente.\n",
    "\n",
    "Indica un valor numérico acompañado de caracteres alfanuméricos de unidad de medida. En este caso siendo \"min\" la unidad de medida para medir la duración.\n",
    "\n",
    "Asumiendo que \"min\" es una constante, debemos dejar en este campo solo valores numéricos y los espacios y caracteres adicionales como \"min\" se deben eliminar.\n",
    "\n",
    "Si encontramos valores nulos en esta columna, vamos a transformarlos a 0.\n",
    "\n",
    "Todo esto debe aplicarse sobre la misma columna \"duration\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7ceee2-ab29-4853-bafc-276d3e2e6123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, when\n",
    "\n",
    "# Limpiar la columna \"duration\" eliminando caracteres no numéricos y convirtiendo valores nulos a 0\n",
    "# Usaremos la expresión regular [^0-9] para indicar que todo lo que no sean números, sea reemplazado por \"\"\n",
    "# La función when nos permite expresar que siempre que el valor sea = \"\", se lleve un 0, y en cualquier otro caso, se deje el mismo valor de la columna\n",
    "# Finalmente usamos la función cast para convertir a Integer el DateType de la columna duration \n",
    "df = df.withColumn(\"duration\", regexp_replace(\"duration\", \"[^0-9]\", \"\"))\n",
    "df = df.withColumn(\"duration\", when(df[\"duration\"].isNull() , 0).otherwise(df[\"duration\"]).cast(\"int\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5557023b-b8a8-48ba-ac1b-329611607a14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 3.3 Columna \"rating\" (Calificación)\n",
    "\n",
    "Hemos identificado que la columna \"rating\" contiene valores nulos.\n",
    "\n",
    "Para esta ocación podríamos cambiar los nulos por 0.\n",
    "\n",
    "Sin embargo, como queremos utilizar esta columna más adelante, podemos simplemente eliminar estos registros con rating == 0.\n",
    "\n",
    "Dejaremos nuestro dataset sin registros con rating null para evitar outliers e información desbalanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c756557-07ab-49b3-bd63-f609a6670a31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# En celdas de código anteriores, hemos importado funciones como when y por ende no hace falta importarlas nuevamente\n",
    "df = df.filter(df[\"rating\"].isNotNull())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f4a6086-b17c-4436-84aa-e88a7aa6f77c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### *A nivel de calidad de datos podemos seguir implementando transformaciones, sin embargo para avanzar con un poco más de ejericios, vamos a dejar nuestro set de datos aquí*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0aac862-3954-4ae5-8e47-ef36f71fab60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## *4. Creación de tabla refinada*\n",
    "\n",
    "A este punto, hemos refinado nuestros datos aplicando diferentes transformaciones orientadas a calidad de datos.\n",
    "\n",
    "Sin embargo, todo lo hemos aplicado a un objeto de tipo DATAFRAME.\n",
    "\n",
    "A continuación, crearemos una tabla que se almacenará en el metastore de Databricks y la llamaremos \"netflix_refined\", indicando así que es la tabla de contenido de netflix ya refinada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f26a3b-e277-41fb-b22c-029509d11b23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 4.1 Almacenamiento en la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17726671-c4cb-48a8-816d-814205190582",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"default.netflix_refined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "951abfb9-2fd2-464f-b737-5a7b2ee79e33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## *5. Lectura de nuestra tabla refinada usando Spark SQL*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b602e949-a1c9-4d7f-9258-7d392aec457f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5.1 Leemos la tabla usando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa31e62-0bf3-4c33-8f23-aa35bfc19892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "Select\n",
    "*\n",
    "from default.netflix_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7b43115-cd08-4f83-9325-650ab689dd67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5.2 Utilizando Spark SQL y directamente un Query, analicemos el rating promedio del contenido de netflix usando la columna de MainGenre que hemos creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2410ba3f-34cb-4def-adb4-1513220ef5c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT\n",
    "t1.MainGenre,\n",
    "round(avg(t1.rating),2) as Promedio\n",
    "FROM default.netflix_refined t1\n",
    "GROUP BY t1.MainGenre\n",
    "order by 2 DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d690fc87-ea6d-4fd8-8ac4-e38ee46789aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5.3 Graficaremos el query anterior para tener una forma visual más clara de analizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e7a841-9e5f-4223-97fc-9d87d033ca98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ejecutar la consulta SQL y almacenar los resultados en un DataFrame de Pandas\n",
    "query_result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        t1.MainGenre,\n",
    "        ROUND(AVG(t1.rating), 2) AS Promedio\n",
    "    FROM\n",
    "        default.netflix_refined t1\n",
    "    WHERE\n",
    "        t1.MainGenre IS NOT NULL\n",
    "    GROUP BY\n",
    "        t1.MainGenre\n",
    "    ORDER BY\n",
    "        Promedio DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Crear un gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(query_result['MainGenre'], query_result['Promedio'], color='skyblue')\n",
    "plt.title('Promedio del rating por género')\n",
    "plt.xlabel('Género')\n",
    "plt.ylabel('Promedio del rating')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033d852a-48b6-4c5a-93d9-7c3e68f09657",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un gráfico de puntos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(query_result['MainGenre'], query_result['Promedio'], marker='o', linestyle='', markersize=8, color='skyblue')\n",
    "plt.title('Promedio del rating por género')\n",
    "plt.xlabel('Género')\n",
    "plt.ylabel('Promedio del rating')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3431198887904223,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sample-03",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
